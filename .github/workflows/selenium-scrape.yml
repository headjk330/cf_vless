name: Selenium Scrape Cloudflare IPs
on:
  workflow_dispatch:  # 手动触发
 # schedule:
   # - cron: '*/15 * * * *'  # 定时触发（每15分钟）

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: 1. 拉取仓库代码
        uses: actions/checkout@v4

      - name: 2. 安装Python环境
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # ------------------- 新增：缓存pip依赖 -------------------
      - name: 3. 缓存Python依赖（避免重复安装）
        uses: actions/cache@v3
        with:
          # 缓存路径：Ubuntu系统中pip依赖默认存放在~/.cache/pip
          path: ~/.cache/pip
          # 缓存键：由“操作系统+pip+requirements.txt哈希值”组成
          # 仅当requirements.txt内容变化时，才会重新生成缓存
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          # 恢复键：若精确匹配失败，可使用同操作系统的旧缓存（可选，提升命中率）
          restore-keys: |
            ${{ runner.os }}-pip-
      # ---------------------------------------------------------

      - name: 4. 安装依赖包（缓存命中时，直接跳过下载，仅验证安装）
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: 5. 安装Chrome浏览器（Linux）
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser

      - name: 6. 运行Selenium爬虫脚本
        run: python scrape_ips.py

      - name: 7. 上传结果文件
        uses: actions/upload-artifact@v4
        with:
          name: cloudflare-ips-result
          path: ips_result.txt
